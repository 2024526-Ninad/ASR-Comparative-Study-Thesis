{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208cbce8-9e01-4bf6-b6a3-13c798400570",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets jiwer torch torchaudio soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384ea303-2deb-4f8e-ba73-6e00989ecfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from jiwer import wer as jiwer_wer, cer as jiwer_cer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "def resample_if_needed(batch, target_sr=16_000):\n",
    "    \"\"\"Hugging Face `Audio(decode=True)` gives numpy float32 and sample_rate. Resample if needed.\"\"\"\n",
    "    wav = batch[\"audio\"][\"array\"]\n",
    "    sr  = batch[\"audio\"][\"sampling_rate\"]\n",
    "    if sr != target_sr:\n",
    "        wav = torchaudio.functional.resample(\n",
    "            torch.from_numpy(wav), sr, target_sr\n",
    "        ).numpy()\n",
    "    batch[\"audio_16k\"] = wav\n",
    "    return batch\n",
    "\n",
    "def normalize_text(s: str):\n",
    "    \"\"\"Lightweight normalizer so WER/CER are comparable across models.\"\"\"\n",
    "    # lower + strip; remove leading/trailing punctuation lumps but keep digits/letters/spaces\n",
    "    s = s.lower().strip()\n",
    "    # collapse whitespace\n",
    "    s = \" \".join(s.split())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c87324-2d4e-4ebe-8ac0-d477b78271bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b6cfd4cdc04e098782792c1607e3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b24392ed6b481a94d76118d48ed8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: 2620   audio: 324.21 min\n",
      "WER: 1.86%\n"
     ]
    }
   ],
   "source": [
    "# ==== Wav2Vec2: LibriSpeech test (accuracy-first) ====\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from jiwer import wer\n",
    "\n",
    "# ---- Config ----\n",
    "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID  = \"facebook/wav2vec2-large-960h-lv60-self\"   # English CTC model\n",
    "TARGET_SR = 16000\n",
    "SPLIT     = \"test[:100%]\"   # change to \"test[:10%]\" while experimenting\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ---- Load dataset (and let Datasets handle resampling to 16 kHz) ----\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=SPLIT)\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=TARGET_SR))  # <-- resamples on the fly\n",
    "\n",
    "# Optional: simple normalizer to match typical WER setup\n",
    "import re\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z' ]+\", \" \", s)  # keep letters and apostrophes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "references = [normalize_text(ex[\"text\"]) for ex in ds]\n",
    "\n",
    "# ---- Load model + processor ----\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
    "\n",
    "# ---- Inference (greedy CTC) ----\n",
    "def transcribe_one(example):\n",
    "    wav = example[\"audio\"][\"array\"]  # float32, 16 kHz (already resampled)\n",
    "    inputs = processor(wav, sampling_rate=TARGET_SR, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "    text = processor.decode(pred_ids)\n",
    "    return text\n",
    "\n",
    "hypotheses = []\n",
    "audio_seconds_total = 0.0\n",
    "\n",
    "for ex in ds:\n",
    "    hypotheses.append(normalize_text(transcribe_one(ex)))\n",
    "    audio_seconds_total += len(ex[\"audio\"][\"array\"]) / TARGET_SR\n",
    "\n",
    "# ---- Compute WER ----\n",
    "score = wer(references, hypotheses) * 100.0\n",
    "print(f\"Items: {len(ds)}   audio: {audio_seconds_total/60:.2f} min\")\n",
    "print(f\"WER: {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db07b3b9-df5f-4d8e-b19f-56b707d80635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Hindi (FLEURS) — optional =====\n",
    "# MODEL_ID  = \"jonatasgrosman/wav2vec2-large-xlsr-53-hindi\"   # Hindi CTC model\n",
    "# TARGET_SR = 16_000\n",
    "# # Pick the Hindi subset and a small portion for demo\n",
    "# lang_code = \"hi_in\"\n",
    "# fleurs = load_dataset(\"google/fleurs\", lang_code, split=\"test[:100%]\")\n",
    "# fleurs = fleurs.cast_column(\"audio\", Audio(decode=True))\n",
    "# fleurs = fleurs.map(resample_if_needed, fn_kwargs={\"target_sr\": TARGET_SR})\n",
    "# \n",
    "# # make it look like LibriSpeech fields expected below\n",
    "# ds = fleurs.rename_columns({\"transcription\": \"text\"})\n",
    "# references = [normalize_text(x[\"text\"]) for x in ds]\n",
    "# \n",
    "# processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "# model      = Wav2Vec2ForCTC.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
    "# \n",
    "# print(\"Items:\", len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6933969c-c724-4cb4-833b-2e473bcc24cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee2c33ab3924f9a823e0006000b5a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a83583790b4a5eac78160f353bb824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: 2620   audio: 19452.48s  (~324.21 min)   wall: 1321.34s\n",
      "WER:  1.86%   RTF: 0.068\n"
     ]
    }
   ],
   "source": [
    "# === Setup ===\n",
    "import torch, numpy as np, re, time\n",
    "from datasets import load_dataset, Audio\n",
    "from jiwer import wer\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "SPLIT    = \"test[:100%]\"     # change to test[:10%] for a quick run\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# === Load LibriSpeech and let 'Audio' do resampling to 16 kHz ===\n",
    "TARGET_SR = 16_000\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=SPLIT)\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=TARGET_SR))  # resample-on-the-fly\n",
    "\n",
    "# === Normalizer to mirror typical WER setups ===\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z' ]+\", \" \", s)  # keep letters + apostrophes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "references = [normalize_text(x[\"text\"]) for x in ds]\n",
    "\n",
    "# === Load processor + model ===\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model     = Wav2Vec2ForCTC.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
    "\n",
    "# === Greedy inference helper ===\n",
    "@torch.no_grad()\n",
    "def transcribe_one(example):\n",
    "    wav = example[\"audio\"][\"array\"]               # float32, 16kHz\n",
    "    inputs = processor(wav, sampling_rate=TARGET_SR, return_tensors=\"pt\").to(DEVICE)\n",
    "    logits  = model(**inputs).logits              # [B, T, vocab]\n",
    "    pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "    text = processor.decode(pred_ids)\n",
    "    return text\n",
    "\n",
    "# === Run inference & eval ===\n",
    "start = time.time()\n",
    "hyps = []\n",
    "audio_sec_total = 0.0\n",
    "\n",
    "for ex in ds:\n",
    "    hyp = transcribe_one(ex)\n",
    "    hyps.append(normalize_text(hyp))\n",
    "    audio_sec_total += len(ex[\"audio\"][\"array\"]) / TARGET_SR\n",
    "\n",
    "mins = audio_sec_total / 60.0\n",
    "score = wer(references, hyps) * 100.0\n",
    "wall  = time.time() - start\n",
    "\n",
    "print(f\"Items: {len(ds)}   audio: {audio_sec_total:.2f}s  (~{mins:.2f} min)   wall: {wall:.2f}s\")\n",
    "print(f\"WER:  {score:.2f}%   RTF: {wall / audio_sec_total:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c8e72dd-49bc-4f06-98cd-310570dc9819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2ea3a1039c4135950b13c4e7bdf2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7e8c0c37e2494a900d330d61a4c08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: 262  audio: 32.41 min  wall: 205.00s  RTF: 0.105\n",
      "WER: 10.64%\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np, re, time\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from jiwer import wer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "SPLIT = \"test[:10%]\"       # use full 'test' when you're ready\n",
    "TARGET_SR = 16_000\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# --- dataset (let Datasets resample on-the-fly) ---\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=SPLIT)\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=TARGET_SR))   # <— IMPORTANT\n",
    "\n",
    "# --- clean refs (to match your Whisper WER setup) ---\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z' ]+\", \" \", s)   # keep letters + apostrophes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "references = [normalize_text(x[\"text\"]) for x in ds]\n",
    "\n",
    "# --- models ---\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model     = Wav2Vec2ForCTC.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
    "\n",
    "# --- batched inference ---\n",
    "BATCH_SIZE = 8\n",
    "hypotheses = []\n",
    "audio_seconds_total = 0.0\n",
    "\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(ds), BATCH_SIZE):\n",
    "        batch = ds.select(range(i, min(i+BATCH_SIZE, len(ds))))\n",
    "\n",
    "        # ❶ pass a LIST of 1-D arrays to the processor (do NOT build a 2-D tensor yourself)\n",
    "        wavs = [ex[\"audio\"][\"array\"] for ex in batch]\n",
    "        audio_seconds_total += sum(len(w) for w in wavs) / TARGET_SR\n",
    "\n",
    "        inputs = processor(\n",
    "            wavs,\n",
    "            sampling_rate=TARGET_SR,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "        logits = model(**inputs).logits                 # [B, T, vocab]\n",
    "        pred_ids = torch.argmax(logits, dim=-1)         # [B, T]\n",
    "        texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        hypotheses.extend(normalize_text(t) for t in texts)\n",
    "\n",
    "wall = time.time() - t0\n",
    "rtf = wall / audio_seconds_total\n",
    "print(f\"Items: {len(ds)}  audio: {audio_seconds_total/60:.2f} min  wall: {wall:.2f}s  RTF: {rtf:.3f}\")\n",
    "\n",
    "score = wer(references, hypotheses) * 100.0\n",
    "print(f\"WER: {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fa57b-cfb6-443c-a59d-35f46f2baf92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (audio)",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
