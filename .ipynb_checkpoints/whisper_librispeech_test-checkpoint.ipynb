{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fca14da-c8fd-40c6-8661-a353f8fa5e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (4.55.3)\n",
      "Requirement already satisfied: torchaudio in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: evaluate in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (0.4.5)\n",
      "Requirement already satisfied: jiwer in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: soundfile in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (0.13.1)\n",
      "Requirement already satisfied: filelock in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
      "Requirement already satisfied: torch==2.8.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torchaudio) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torch==2.8.0->torchaudio) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torch==2.8.0->torchaudio) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torch==2.8.0->torchaudio) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torch==2.8.0->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: click>=8.1.8 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from jiwer) (8.2.1)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: pycparser in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from jinja2->torch==2.8.0->torchaudio) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers torchaudio evaluate jiwer soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d453ddd-3501-4947-8378-b2fd7062f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e364f6e4-c94b-4ebd-b70b-dbf0776186ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: torch==2.8.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torchaudio) (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torch==2.8.0->torchaudio) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torch==2.8.0->torchaudio) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torch==2.8.0->torchaudio) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torch==2.8.0->torchaudio) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torch==2.8.0->torchaudio) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torch==2.8.0->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from torch==2.8.0->torchaudio) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (from jinja2->torch==2.8.0->torchaudio) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da71f787-84d8-4ac5-8f20-f4d1b1a24350",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/pytorch/audio.git@main#egg=torchcodec&subdirectory=torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b4b139-694e-4917-8cfc-c9fe49aded54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchcodec in /Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages (0.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "266a4b28-7ed8-4b71-b6a5-03e216feb3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ninadjoshi/.local/share/mamba/envs/audio/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "# Step 0: Force download of the actual audio file\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test[:1]\")\n",
    "_ = ds[0][\"audio\"]  # this downloads the FLAC file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec5e55a-58dc-4998-81b6-59113296b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<datasets.features._torchcodec.AudioDecoder object at 0x15a8bc5f0>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Step 1: Download with decode=True to get the actual file\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test[:1]\")\n",
    "audio_info = ds[0][\"audio\"]\n",
    "print(audio_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e86eb918-f96d-4d47-be7f-938d97c2f917",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torchcodec.decoders.AudioDecoder' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(os.path.exists(\u001b[43maudio_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/datasets/features/_torchcodec.py:15\u001b[39m, in \u001b[36mAudioDecoder.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getitem__\u001b[39m(key)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtorchcodec.decoders.AudioDecoder\u001b[39m\u001b[33m'\u001b[39m\u001b[33m object is not subscriptable\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'torchcodec.decoders.AudioDecoder' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(audio_info[\"path\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7255db23-cc0f-460a-9521-eb94003da2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio path: 6930-75918-0000.flac\n",
      "Exists? False\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "import os\n",
    "\n",
    "# Step 1: Load the dataset with decode=False to access raw path\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test[:1]\")\n",
    "ds = ds.cast_column(\"audio\", Audio(decode=False))\n",
    "\n",
    "# Step 2: Extract path\n",
    "sample = ds[0]\n",
    "audio_path = sample[\"audio\"][\"path\"]\n",
    "\n",
    "# Step 3: Verify file exists\n",
    "print(\"Audio path:\", audio_path)\n",
    "print(\"Exists?\", os.path.exists(audio_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46fb194-4646-4581-bab9-0e2d65816f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [05:34<00:00,  6.97s/files]\n",
      "Generating test split: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2620/2620 [00:00<00:00, 8716.78 examples/s]\n",
      "Generating train.100 split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28539/28539 [00:03<00:00, 7638.10 examples/s]\n",
      "Generating train.360 split: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104014/104014 [00:13<00:00, 7582.49 examples/s]\n",
      "Generating validation split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2703/2703 [00:00<00:00, 14193.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Force redownload the actual .flac file\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test[:1]\", download_mode=\"force_redownload\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fecc857-0eaf-434d-af50-2a7bebac5cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [05:29<00:00,  6.87s/files]\n",
      "Generating test split: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2620/2620 [00:00<00:00, 12786.10 examples/s]\n",
      "Generating train.100 split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28539/28539 [00:03<00:00, 8879.74 examples/s]\n",
      "Generating train.360 split: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104014/104014 [00:12<00:00, 8404.06 examples/s]\n",
      "Generating validation split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2703/2703 [00:00<00:00, 11829.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\n",
    "    \"librispeech_asr\", \"clean\",\n",
    "    split=\"test[:1]\",\n",
    "    download_mode=\"force_redownload\"  # one more push if cache was half-done\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb333721-3b1c-47ac-b611-9504a9cdb633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56080,) 16000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "# tiny slice to keep it fast\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test[:1]\")\n",
    "\n",
    "# ask datasets to decode the audio for you\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))  # decode=True by default\n",
    "\n",
    "ex = ds[0][\"audio\"]\n",
    "waveform = ex[\"array\"]      # numpy float32, shape [T]\n",
    "sample_rate = ex[\"sampling_rate\"]\n",
    "\n",
    "print(waveform.shape, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61f2c756-7499-411d-a944-4ab0999e808b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 56080]) 16000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "import torch\n",
    "\n",
    "# Tiny slice to keep it fast\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test[:1]\")\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))  # decode=True by default\n",
    "\n",
    "ex = ds[0]\n",
    "waveform = torch.from_numpy(ex[\"audio\"][\"array\"]).unsqueeze(0)  # [1, T] torch float32\n",
    "sample_rate = ex[\"audio\"][\"sampling_rate\"]\n",
    "print(waveform.shape, sample_rate)   # e.g., torch.Size([1, 56080]) 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e3ed220-1ba9-4b18-bcc7-235d292896d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers accelerate evaluate jiwer sentencepiece --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de81a6f1-38b8-4dfc-b198-0a2d568c4a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 56080]) 16000\n",
      "REF: CONCORD RETURNED TO ITS PLACE AMIDST THE TENTS\n"
     ]
    }
   ],
   "source": [
    "import torch, time, re, numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "# Rebuild the exact tiny example you just used (decoded to 16 kHz)\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test[:1]\")\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "ex = ds[0]\n",
    "wave_np = ex[\"audio\"][\"array\"].astype(\"float32\")      # [T]\n",
    "wave_pt = torch.from_numpy(wave_np).unsqueeze(0)      # [1, T]\n",
    "sr = ex[\"audio\"][\"sampling_rate\"]\n",
    "ref_text = ex[\"text\"]\n",
    "\n",
    "print(wave_pt.shape, sr)\n",
    "print(\"REF:\", ref_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80d7205e-f446-4239-93f4-43ea3317bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9' ]+\", \" \", s)  # keep letters, digits, apostrophes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9294e47-6d35-402c-a1ec-67d9eb0aaede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHISPER:  Concord returned to its place amidst the tents.\n",
      "Whisper time: 11.752s   audio: 3.505s   RTF: 3.353\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id_whisper = \"openai/whisper-medium.en\"   # or: openai/whisper-large-v3\n",
    "\n",
    "processor_w = WhisperProcessor.from_pretrained(model_id_whisper)\n",
    "model_w = WhisperForConditionalGeneration.from_pretrained(model_id_whisper).to(device)\n",
    "model_w.eval()\n",
    "\n",
    "# Force English transcription (no translation)\n",
    "forced_ids = processor_w.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = processor_w(wave_np, sampling_rate=sr, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Inference\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model_w.generate(\n",
    "        **inputs,\n",
    "        forced_decoder_ids=forced_ids,\n",
    "        # for accuracy you can let it search more (slower):\n",
    "        num_beams=5, length_penalty=1.0, no_repeat_ngram_size=3\n",
    "    )\n",
    "pred_whisper = processor_w.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "t1 = time.time()\n",
    "\n",
    "dur = len(wave_np) / sr\n",
    "rtf_w = (t1 - t0) / dur\n",
    "print(\"WHISPER:\", pred_whisper)\n",
    "print(f\"Whisper time: {t1 - t0:.3f}s   audio: {dur:.3f}s   RTF: {rtf_w:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcc0b9de-d6f8-4613-9819-bdf0f5aba802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading openai/whisper-large-v3 on cpu …\n",
      "\n",
      "Whisper model: openai/whisper-large-v3\n",
      "Items: 50  audio: 328.27s  wall: 899.24s\n",
      "WER: 0.0295   CER: 0.0061   RTF: 2.739\n"
     ]
    }
   ],
   "source": [
    "import time, torch, numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from jiwer import wer, cer, Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces, Strip\n",
    "\n",
    "# ---------- config ----------\n",
    "PREFER_ACCURACY = True     # your preference\n",
    "MAX_ITEMS = 50             # evaluate on first N test samples (raise later)\n",
    "SR = 16000\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# best → … → lighter\n",
    "CANDIDATE_MODELS = [\n",
    "    \"openai/whisper-large-v3\",\n",
    "    \"openai/whisper-large-v2\",\n",
    "    \"openai/whisper-medium.en\"\n",
    "] if PREFER_ACCURACY else [\"openai/whisper-medium.en\"]\n",
    "\n",
    "def load_whisper():\n",
    "    last_err = None\n",
    "    for name in CANDIDATE_MODELS:\n",
    "        try:\n",
    "            print(f\"Loading {name} on {DEVICE} …\")\n",
    "            proc = WhisperProcessor.from_pretrained(name)\n",
    "            model = WhisperForConditionalGeneration.from_pretrained(\n",
    "                name,\n",
    "                torch_dtype=torch.float16 if DEVICE==\"cuda\" else None\n",
    "            ).to(DEVICE)\n",
    "            model.eval()\n",
    "            return name, proc, model\n",
    "        except RuntimeError as e:\n",
    "            last_err = e\n",
    "            print(f\"OOM or load error with {name}: {e}\")\n",
    "            torch.cuda.empty_cache() if DEVICE==\"cuda\" else None\n",
    "    raise last_err\n",
    "\n",
    "model_id, processor, model_w = load_whisper()\n",
    "\n",
    "# generation settings emphasising accuracy (beam search, low temp)\n",
    "GEN_KW = dict(\n",
    "    language=\"en\", task=\"transcribe\",\n",
    "    num_beams=8, length_penalty=1.0,\n",
    "    temperature=0.0, no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.05\n",
    ")\n",
    "\n",
    "# ---------- data ----------\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test[:{}]\".format(MAX_ITEMS))\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=SR))\n",
    "\n",
    "# text normalization for fair WER/CER\n",
    "norm = Compose([ToLowerCase(), RemovePunctuation(), RemoveMultipleSpaces(), Strip()])\n",
    "\n",
    "# ---------- eval loop ----------\n",
    "preds, refs = [], []\n",
    "audio_seconds_total = 0.0\n",
    "t0 = time.time()\n",
    "\n",
    "for ex in ds:\n",
    "    wav = ex[\"audio\"][\"array\"]          # float32 numpy [T]\n",
    "    sr  = ex[\"audio\"][\"sampling_rate\"]\n",
    "    if sr != SR:\n",
    "        # (should already be 16 kHz from cast_column, but just in case)\n",
    "        # torchaudio resample path if you really need it:\n",
    "        # wav = torchaudio.functional.resample(torch.tensor(wav), sr, SR).numpy()\n",
    "        pass\n",
    "\n",
    "    audio_seconds_total += len(wav)/SR\n",
    "\n",
    "    inputs = processor(\n",
    "        wav, sampling_rate=SR, return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_ids = model_w.generate(**inputs, **GEN_KW)\n",
    "\n",
    "    text = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "    ref  = ex[\"text\"]\n",
    "\n",
    "    preds.append(norm(text))\n",
    "    refs.append(norm(ref))\n",
    "\n",
    "wall = time.time() - t0\n",
    "WER = wer(refs, preds)\n",
    "CER = cer(refs, preds)\n",
    "RTF = wall / audio_seconds_total\n",
    "\n",
    "print(f\"\\nWhisper model: {model_id}\")\n",
    "print(f\"Items: {len(refs)}  audio: {audio_seconds_total:.2f}s  wall: {wall:.2f}s\")\n",
    "print(f\"WER: {WER:.4f}   CER: {CER:.4f}   RTF: {RTF:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1db404f-f7a3-4025-a3c4-f70c97591880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wav2Vec2 model: facebook/wav2vec2-large-960h-lv60-self\n",
      "Items: 50  audio: 328.27s  wall: 18.23s\n",
      "WER: 0.0164   CER: 0.0061   RTF: 0.056\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from jiwer import wer, cer, Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces, Strip\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SR = 16000\n",
    "MAX_ITEMS = 50\n",
    "\n",
    "# High‑accuracy English model (CTC):\n",
    "wav2vec_id = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "\n",
    "proc2 = Wav2Vec2Processor.from_pretrained(wav2vec_id)\n",
    "model2 = Wav2Vec2ForCTC.from_pretrained(wav2vec_id).to(DEVICE)\n",
    "model2.eval()\n",
    "\n",
    "ds2 = load_dataset(\"librispeech_asr\", \"clean\", split=\"test[:{}]\".format(MAX_ITEMS))\n",
    "ds2 = ds2.cast_column(\"audio\", Audio(sampling_rate=SR))\n",
    "\n",
    "norm = Compose([ToLowerCase(), RemovePunctuation(), RemoveMultipleSpaces(), Strip()])\n",
    "\n",
    "preds, refs = [], []\n",
    "audio_seconds_total = 0.0\n",
    "t0 = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ex in ds2:\n",
    "        wav = ex[\"audio\"][\"array\"]\n",
    "        audio_seconds_total += len(wav)/SR\n",
    "        inputs = proc2(wav, sampling_rate=SR, return_tensors=\"pt\", padding=\"longest\").to(DEVICE)\n",
    "        logits = model2(**inputs).logits\n",
    "        ids = torch.argmax(logits, dim=-1)\n",
    "        text = proc2.batch_decode(ids)[0]\n",
    "        preds.append(norm(text))\n",
    "        refs.append(norm(ex[\"text\"]))\n",
    "\n",
    "wall = time.time() - t0\n",
    "WER = wer(refs, preds)\n",
    "CER = cer(refs, preds)\n",
    "RTF = wall / audio_seconds_total\n",
    "\n",
    "print(f\"\\nWav2Vec2 model: {wav2vec_id}\")\n",
    "print(f\"Items: {len(refs)}  audio: {audio_seconds_total:.2f}s  wall: {wall:.2f}s\")\n",
    "print(f\"WER: {WER:.4f}   CER: {CER:.4f}   RTF: {RTF:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d4af005-0f0a-49bd-8561-14084e4f1eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2620 utterances\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['beam_size', 'best_of'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     42\u001b[39m audio_sec_total += \u001b[38;5;28mlen\u001b[39m(wav) / sr\n\u001b[32m     44\u001b[39m inputs = processor(\n\u001b[32m     45\u001b[39m     wav, sampling_rate=sr, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m ).to(DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m gen_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mGEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m text = processor.batch_decode(gen_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     51\u001b[39m preds.append(norm(text))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:866\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, monitor_progress, **kwargs)\u001b[39m\n\u001b[32m    857\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[32m    860\u001b[39m (\n\u001b[32m    861\u001b[39m     seek_sequences,\n\u001b[32m    862\u001b[39m     seek_outputs,\n\u001b[32m    863\u001b[39m     should_skip,\n\u001b[32m    864\u001b[39m     do_condition_on_prev_tokens,\n\u001b[32m    865\u001b[39m     model_output_type,\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:1038\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, seek, batch_idx_map, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1034\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n\u001b[32m   1035\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n\u001b[32m   1036\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1050\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[32m   1052\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/transformers/generation/utils.py:2367\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2362\u001b[39m assistant_tokenizer = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33massistant_tokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[32m   2364\u001b[39m generation_config, model_kwargs = \u001b[38;5;28mself\u001b[39m._prepare_generation_config(\n\u001b[32m   2365\u001b[39m     generation_config, use_model_defaults, **kwargs\n\u001b[32m   2366\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2367\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2368\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[32m   2370\u001b[39m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/transformers/generation/utils.py:1589\u001b[39m, in \u001b[36mGenerationMixin._validate_model_kwargs\u001b[39m\u001b[34m(self, model_kwargs)\u001b[39m\n\u001b[32m   1586\u001b[39m         unused_model_args.append(key)\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[32m-> \u001b[39m\u001b[32m1589\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1590\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (note: typos in the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1591\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m generate arguments will also show up in this list)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1592\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The following `model_kwargs` are not used by the model: ['beam_size', 'best_of'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# --- Accuracy-first Whisper evaluation on LibriSpeech test-clean ---\n",
    "import time, torch, numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from jiwer import wer, cer, Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces, Strip\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/whisper-large-v3\"\n",
    "\n",
    "# 1) Data\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))  # ensures 16 kHz\n",
    "print(f\"{len(ds)} utterances\")\n",
    "\n",
    "# 2) Model\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_ID)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 3) Text normalization for scoring (roughly Whisper style)\n",
    "norm = Compose([ToLowerCase(), RemovePunctuation(), RemoveMultipleSpaces(), Strip()])\n",
    "\n",
    "# 4) Generation config tuned for accuracy (beam search)\n",
    "GEN = dict(\n",
    "    task=\"transcribe\",\n",
    "    language=\"en\",\n",
    "    temperature=0.0,        # greedy within beam\n",
    "    beam_size=5,            # modest beam, good accuracy/speed balance\n",
    "    best_of=5,              # consider several hypotheses before beam\n",
    "    length_penalty=1.0,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "preds, refs = [], []\n",
    "audio_sec_total = 0.0\n",
    "t0 = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ex in ds:  # full set; slice if needed e.g. ds.select(range(1000))\n",
    "        wav = ex[\"audio\"][\"array\"]             # float32 [T]\n",
    "        sr  = ex[\"audio\"][\"sampling_rate\"]\n",
    "        audio_sec_total += len(wav) / sr\n",
    "\n",
    "        inputs = processor(\n",
    "            wav, sampling_rate=sr, return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        gen_ids = model.generate(**inputs, **GEN)\n",
    "        text = processor.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        preds.append(norm(text))\n",
    "        refs.append(norm(ex[\"text\"]))\n",
    "\n",
    "t1 = time.time()\n",
    "WER = wer(refs, preds)\n",
    "CER_v = cer(refs, preds)\n",
    "RTF = (t1 - t0) / audio_sec_total\n",
    "\n",
    "print(f\"Whisper large-v3  | items: {len(refs)}  audio: {audio_sec_total:.2f}s  wall: {t1-t0:.2f}s\")\n",
    "print(f\"WER: {WER:.4f}  CER: {CER_v:.4f}  RTF: {RTF:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a51717d-6ab7-4503-9b4f-1f49d3fd7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = model2(**inputs).logits\n",
    "# ids = torch.argmax(logits, dim=-1)\n",
    "# text = proc2.batch_decode(ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38a008d0-411c-443a-9d0c-fa13010b009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60 and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RepositoryNotFoundError",
     "evalue": "401 Client Error. (Request ID: Root=1-68b177f0-460c6a1755eb128842f8c180;f544f797-4566-4885-9018-bd593a1bc4a3)\n\nRepository Not Found for url: https://huggingface.co/kensho-ai/kenlm/resolve/main/4gram.arpa.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/kensho-ai/kenlm/resolve/main/4gram.arpa",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m model.eval()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 3) KenLM (download a 4-gram ARPA from HF; any good English 4-gram works)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m arpa_path = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkensho-ai/kenlm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m4gram.arpa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# swap if you prefer a different LM\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Build decoder with the model’s CTC vocabulary\u001b[39;00m\n\u001b[32m     26\u001b[39m vocab_list = \u001b[38;5;28mlist\u001b[39m(proc.tokenizer.get_vocab().keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/huggingface_hub/file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    991\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    992\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1007\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1008\u001b[39m     )\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/huggingface_hub/file_download.py:1117\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1114\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1116\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m etag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33metag must have been retrieved from server\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/huggingface_hub/file_download.py:1658\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[32m   1650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1651\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m hf.co look-ups and downloads online, set \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlocal_files_only\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1652\u001b[39m     )\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1654\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1655\u001b[39m ):\n\u001b[32m   1656\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[32m   1661\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[32m   1662\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1663\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1664\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is on.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1665\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhead_call_error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/huggingface_hub/file_download.py:1546\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1544\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1545\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m         metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1550\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1551\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/huggingface_hub/file_download.py:1463\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1460\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1474\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/huggingface_hub/file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m300\u001b[39m <= response.status_code <= \u001b[32m399\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/huggingface_hub/file_download.py:310\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m    309\u001b[39m response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(\u001b[32m429\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:459\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_code == \u001b[33m\"\u001b[39m\u001b[33mRepoNotFound\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    439\u001b[39m     response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m    440\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m error_message != \u001b[33m\"\u001b[39m\u001b[33mInvalid credentials in Authorization header\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[32m    449\u001b[39m     \u001b[38;5;66;03m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n\u001b[32m    450\u001b[39m     message = (\n\u001b[32m    451\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n\u001b[32m    462\u001b[39m     message = (\n\u001b[32m    463\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m endpoint:\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBad request:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    464\u001b[39m     )\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m: 401 Client Error. (Request ID: Root=1-68b177f0-460c6a1755eb128842f8c180;f544f797-4566-4885-9018-bd593a1bc4a3)\n\nRepository Not Found for url: https://huggingface.co/kensho-ai/kenlm/resolve/main/4gram.arpa.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "# --- Accuracy-first Wav2Vec2-CTC with KenLM decoding ---\n",
    "!pip -q install pyctcdecode==0.5.0 kenlm\n",
    "\n",
    "import time, torch, numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from jiwer import wer, cer, Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces, Strip\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "W2V_ID = \"facebook/wav2vec2-large-960h-lv60\"  # or \"jonatasgrosman/wav2vec2-large-robust-ft-libri-960h\"\n",
    "\n",
    "# 1) Data\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# 2) Model\n",
    "proc = Wav2Vec2Processor.from_pretrained(W2V_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(W2V_ID).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 3) KenLM (download a 4-gram ARPA from HF; any good English 4-gram works)\n",
    "arpa_path = hf_hub_download(\"kensho-ai/kenlm\", filename=\"4gram.arpa\")  # swap if you prefer a different LM\n",
    "# Build decoder with the model’s CTC vocabulary\n",
    "vocab_list = list(proc.tokenizer.get_vocab().keys())\n",
    "decoder = build_ctcdecoder(vocab_list, arpa_path)\n",
    "\n",
    "# 4) Scoring normalization\n",
    "norm = Compose([ToLowerCase(), RemovePunctuation(), RemoveMultipleSpaces(), Strip()])\n",
    "\n",
    "preds, refs = [], []\n",
    "audio_sec_total = 0.0\n",
    "t0 = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ex in ds:  # full set recommended\n",
    "        wav = ex[\"audio\"][\"array\"]\n",
    "        sr  = ex[\"audio\"][\"sampling_rate\"]\n",
    "        audio_sec_total += len(wav) / sr\n",
    "\n",
    "        inputs = proc(wav, sampling_rate=sr, return_tensors=\"pt\", padding=\"longest\").to(DEVICE)\n",
    "        #logits = model(**inputs).logits[0].cpu().numpy()\n",
    "        # LM decode (beam width ~100 is common; increase for accuracy)\n",
    "        #text = decoder.decode(logits, beam_width=100)\n",
    "\n",
    "        preds.append(norm(text))\n",
    "        refs.append(norm(ex[\"text\"]))\n",
    "\n",
    "t1 = time.time()\n",
    "WER = wer(refs, preds)\n",
    "CER_v = cer(refs, preds)\n",
    "RTF = (t1 - t0) / audio_sec_total\n",
    "\n",
    "print(f\"Wav2Vec2-CTC+LM | items: {len(refs)}  audio: {audio_sec_total:.2f}s  wall: {t1-t0:.2f}s\")\n",
    "print(f\"WER: {WER:.4f}  CER: {CER_v:.4f}  RTF: {RTF:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "183897c3-c2ea-4536-8945-ba90e5972a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: pyctcdecode==0.5.*\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyctcdecode==0.5.* kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d1399fd-c3a8-4e1b-adf7-220a8601ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1292M  100 1292M    0     0  41.5M      0  0:00:31  0:00:31 --:--:-- 43.3M\n"
     ]
    }
   ],
   "source": [
    "# in a notebook cell\n",
    "!mkdir -p lm\n",
    "!curl -L -o lm/4-gram.arpa.gz https://www.openslr.org/resources/11/4-gram.arpa.gz\n",
    "!gunzip -f lm/4-gram.arpa.gz    # produces lm/4-gram.arpa\n",
    "LM_ARPA = \"lm/4-gram.arpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6da0f619-cf0e-4f67-899c-ea35e312c523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC vocab size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: (175, 32)   expected V: 32\n",
      "CTC beam text: CONCORD RETURNED TO ITS PLACE AMIDST THE TENTS\n",
      "REF: CONCORD RETURNED TO ITS PLACE AMIDST THE TENTS\n"
     ]
    }
   ],
   "source": [
    "import os, torch, numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"facebook/wav2vec2-large-960h-lv60-self\"   # English CTC model (vocab ~32)\n",
    "\n",
    "# 1) Load proper processor + model (CTC)\n",
    "proc_ctc = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "asr_ctc  = Wav2Vec2ForCTC.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
    "\n",
    "# 2) Get the *matching* CTC labels for the decoder\n",
    "vocab_dict = proc_ctc.tokenizer.get_vocab()           # dict: token -> id\n",
    "id2token   = {i: tok for tok, i in vocab_dict.items()}# reorder by id\n",
    "labels     = [id2token[i] for i in range(len(id2token))]\n",
    "print(\"CTC vocab size:\", len(labels))                 # should match asr_ctc.config.vocab_size (≈32)\n",
    "\n",
    "# 3) Tiny LibriSpeech slice, do NOT auto-decode so we keep raw arrays\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test[:1%]\")\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))  # ensure 16k\n",
    "\n",
    "# 4) Build pyctcdecode decoder (no LM first — this alone already improves over argmax)\n",
    "decoder = build_ctcdecoder(labels)                    # kenlm optional; add arpa later\n",
    "\n",
    "# 5) Run one (or loop) and check shapes\n",
    "ex    = ds[0]\n",
    "wav   = np.asarray(ex[\"audio\"][\"array\"], dtype=np.float32)\n",
    "inp   = proc_ctc(wav, sampling_rate=16_000, return_tensors=\"pt\", padding=\"longest\").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = asr_ctc(**inp).logits[0].cpu().numpy()  # (T, V)\n",
    "\n",
    "print(\"logits shape:\", logits.shape, \"  expected V:\", len(labels))\n",
    "assert logits.shape[1] == len(labels), \"Vocab size mismatch — decoder built with wrong labels.\"\n",
    "\n",
    "# 6) Beam search decoding (no LM)\n",
    "text_beam = decoder.decode(logits)\n",
    "print(\"CTC beam text:\", text_beam)\n",
    "\n",
    "# Optional: compute WER against ref if you have your `norm()` handy\n",
    "ref_text = ex[\"text\"]\n",
    "print(\"REF:\", ref_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c0cf914-6073-4d72-adac-ddff4e01a481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper:  Concord returned to its place amidst the tents.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "W_NAME  = \"openai/whisper-large-v3\"\n",
    "\n",
    "proc_w = WhisperProcessor.from_pretrained(W_NAME)\n",
    "whisp  = WhisperForConditionalGeneration.from_pretrained(W_NAME).to(DEVICE).eval()\n",
    "\n",
    "# Prepare one example (16k array -> log-mel happens in processor)\n",
    "wav = ex[\"audio\"][\"array\"]   # from the same ds you used above (16k)\n",
    "inputs = proc_w.feature_extractor(\n",
    "    wav, sampling_rate=16_000, return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "\n",
    "gen_cfg = dict(\n",
    "    language=\"en\", task=\"transcribe\",\n",
    "    num_beams=5, temperature=0.0,        # accuracy > speed\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = whisp.generate(**inputs, **gen_cfg)\n",
    "\n",
    "text_w = proc_w.tokenizer.batch_decode(out.sequences, skip_special_tokens=True)[0]\n",
    "print(\"Whisper:\", text_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf2263-2552-4b76-9382-c60346f5855b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f689145-78dd-4c34-9918-b15e9e76a56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1eb792-d35a-4638-9a83-4054d9eca586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659207a6-36c2-46bb-ade7-728a3a4bc2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310941c6-0b23-4c87-a2b9-70fe4e5aa0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645fecdf-202a-485c-a0de-1273f1ab6312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c57b8a-2d15-4e45-926b-915b41dbbe67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb213c2-4c16-4e91-a0e3-8be943f55075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97cde8-7225-42bd-a6fb-145ad356a1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc7df9-9437-44c8-a0a9-daca2db07f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (audio)",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
