{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f66e3f-4d3d-4b76-9be7-13b7f35261e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6c0c88db2a46edb0183e04c52283d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886c8a399bca4e12ac16b53e019520ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: 262\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "import time, re, torch\n",
    "from datasets import load_dataset, Audio\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from jiwer import wer, cer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TARGET_SR = 16_000\n",
    "SPLIT = \"test[:10%]\"      # change to \"test\" for full set, or \"test[:1%]\" for a quick run\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# --- Load LibriSpeech and resample on-the-fly to 16 kHz ---\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=SPLIT)\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=TARGET_SR))\n",
    "\n",
    "# --- Normalizer (same as used in many papers for fair WER) ---\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z' ]+\", \" \", s)      # keep letters + apostrophes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "references = [normalize_text(x[\"text\"]) for x in ds]\n",
    "print(f\"Items: {len(ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eddf100-4aa0-454a-9998-92a558398bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_raw(batch):\n",
    "    # batch: list of dicts with \"audio\" etc.\n",
    "    wavs = [torch.tensor(ex[\"audio\"][\"array\"]) for ex in batch]\n",
    "    lens = [len(w) for w in wavs]\n",
    "    # Keep raw list (each 1D); each model will pad in its own processor\n",
    "    return wavs, lens\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "465fe55b-67b6-4eee-a73a-4a2e5d441e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1e77b87f6a493ab2e5227363c922d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe21704764ca4910bc746707f8eed659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: 262  audio: 32.41 min  wall: 194.46s  RTF: 0.100\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np, re, time\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "SPLIT = \"test[:10%]\"  # adjust\n",
    "\n",
    "# 1) Load data and resample on the fly to 16k for CTC\n",
    "TARGET_SR = 16_000\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=SPLIT)\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=TARGET_SR))\n",
    "\n",
    "# 2) Simple normalizer to match WER setups\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z' ]+\", \" \", s)   # keep letters and apostrophes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "references = [normalize_text(x[\"text\"]) for x in ds]\n",
    "\n",
    "# 3) Load processor + model\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
    "\n",
    "# 4) Batched evaluation\n",
    "BATCH_SIZE = 8\n",
    "hypotheses = []\n",
    "audio_seconds_total = 0.0\n",
    "\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(ds), BATCH_SIZE):\n",
    "        # make a “batch” by slicing the dataset\n",
    "        batch = ds.select(range(i, min(i + BATCH_SIZE, len(ds))))\n",
    "\n",
    "        # *** extract a LIST of 1-D float arrays (not 2-D) ***\n",
    "        wavs = [ex[\"audio\"][\"array\"] for ex in batch]\n",
    "        audio_seconds_total += sum(len(w) for w in wavs) / TARGET_SR\n",
    "\n",
    "        # processor pads and builds tensors\n",
    "        inputs = processor(\n",
    "            wavs,\n",
    "            sampling_rate=TARGET_SR,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True  # pad-to-longest\n",
    "        )\n",
    "\n",
    "        # move to device, run model\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        logits = model(**inputs).logits                       # [B, T, vocab]\n",
    "        pred_ids = torch.argmax(logits, dim=-1)               # [B, T]\n",
    "        texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        hypotheses.extend(normalize_text(t) for t in texts)\n",
    "\n",
    "wall = time.time() - t0\n",
    "rtf = wall / audio_seconds_total\n",
    "\n",
    "print(f\"Items: {len(ds)}  audio: {audio_seconds_total/60:.2f} min  wall: {wall:.2f}s  RTF: {rtf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f15f670-2236-4a78-bd51-aa4599c5739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"facebook/wav2vec2-large-960h-lv60-self\"  # English CTC\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model     = Wav2Vec2ForCTC.from_pretrained(MODEL_ID).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66566625-198b-42c2-9b85-008595c3c064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: 262  audio: 32.41 min  wall: 196.95s  RTF: 0.101\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "hypotheses = []\n",
    "audio_seconds_total = 0.0\n",
    "\n",
    "t0 = time.time()\n",
    "with torch.inference_mode():\n",
    "    for i in range(0, len(ds), BATCH_SIZE):\n",
    "        # Slice the dataset instead of using a DataLoader\n",
    "        batch = ds.select(range(i, min(i + BATCH_SIZE, len(ds))))\n",
    "\n",
    "        # IMPORTANT: pass a LIST of raw 1-D arrays to the processor\n",
    "        wavs = [ex[\"audio\"][\"array\"] for ex in batch]          # list of 1-D float arrays\n",
    "        audio_seconds_total += sum(len(w) for w in wavs) / TARGET_SR\n",
    "\n",
    "        inputs = processor(\n",
    "            wavs,\n",
    "            sampling_rate=TARGET_SR,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,            # let processor pad\n",
    "        )\n",
    "\n",
    "        logits = model(\n",
    "            input_values=inputs.input_values.to(DEVICE),\n",
    "            attention_mask=inputs.attention_mask.to(DEVICE),\n",
    "        ).logits                                   # [B, T, vocab]\n",
    "\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        hypotheses.extend(normalize_text(t) for t in texts)\n",
    "\n",
    "t1 = time.time()\n",
    "wer_score = 100.0 * (\n",
    "    sum(1 for _ in references)  # dummy, replace with your WER function if needed\n",
    ")\n",
    "\n",
    "wall = t1 - t0\n",
    "rtf  = wall / audio_seconds_total  # Real-Time Factor\n",
    "\n",
    "print(f\"Items: {len(ds)}  audio: {audio_seconds_total/60:.2f} min  wall: {wall:.2f}s  RTF: {rtf:.3f}\")\n",
    "# print(f\"WER: {wer_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3671066-b713-4f10-903b-5817f101c9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225a0a0ade454f0b8cb6f2113df6ec33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2de6f544584a1384535038bad47524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import time, torch, numpy as np, re\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "# from jiwer import wer \n",
    "\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "TARGET_SR = 16_000\n",
    "\n",
    "# Load LibriSpeech (resampled on the fly)\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test[:10%]\")  # adjust split if you like\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=TARGET_SR))\n",
    "\n",
    "# simple normalizer (same as you used above)\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z' ]+\", \" \", s)   # keep letters + apostrophes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "references = [normalize_text(x[\"text\"]) for x in ds]\n",
    "\n",
    "# Load processor + model\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model     = Wav2Vec2ForCTC.from_pretrained(MODEL_ID).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a91a4cb-a8da-4e5a-85c1-611778a7886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: 262  audio: 32.41m  wall: 195.92s  RTF: 0.101\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "def transcribe_batch(batch_examples):\n",
    "    # a list of 1-D float32 arrays at 16 kHz\n",
    "    wavs = [ex[\"audio\"][\"array\"] for ex in batch_examples]\n",
    "\n",
    "    #the processor pad and build tensors\n",
    "    inputs = processor(\n",
    "        wavs,\n",
    "        sampling_rate=TARGET_SR,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,              # pad to longest\n",
    "        # pad_to_multiple_of=8,    # optional, a tiny speedup on some CPUs/GPUs\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # forward\n",
    "    with torch.inference_mode():\n",
    "        logits = model(\n",
    "            input_values   = inputs.input_values,\n",
    "            attention_mask = inputs.attention_mask\n",
    "        ).logits                       # [B, T, vocab]\n",
    "        pred_ids = torch.argmax(logits, dim=-1)   # [B, T] -> ids\n",
    "        texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    # normalize transcripts to match refs\n",
    "    return [normalize_text(t) for t in texts]\n",
    "\n",
    "# --- run the loop ---\n",
    "hyps = []\n",
    "audio_seconds_total = 0.0\n",
    "t0 = time.time()\n",
    "\n",
    "for i in range(0, len(ds), BATCH_SIZE):\n",
    "    batch = ds.select(range(i, min(i + BATCH_SIZE, len(ds))))\n",
    "    # keep speed/RTF accounting\n",
    "    wavs = [ex[\"audio\"][\"array\"] for ex in batch]\n",
    "    audio_seconds_total += sum(len(w) for w in wavs) / TARGET_SR\n",
    "    hyps.extend(transcribe_batch(batch))\n",
    "\n",
    "wall = time.time() - t0\n",
    "rtf  = wall / audio_seconds_total\n",
    "\n",
    "print(f\"Items: {len(ds)}  audio: {audio_seconds_total/60:.2f}m  wall: {wall:.2f}s  RTF: {rtf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e65fdcc3-5db1-4002-a1b9-9ca33d79fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: 262  audio: 32.41m  wall: 197.46s  RTF: 0.102\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_keep_lists(examples):\n",
    "    return {\n",
    "        \"wavs\": [ex[\"audio\"][\"array\"] for ex in examples],\n",
    "        \"text\": [ex[\"text\"] for ex in examples]\n",
    "    }\n",
    "\n",
    "loader = DataLoader(ds, batch_size=8, shuffle=False, collate_fn=collate_keep_lists)\n",
    "\n",
    "hyps, audio_seconds_total = [], 0.0\n",
    "t0 = time.time()\n",
    "for batch in loader:\n",
    "    wavs = batch[\"wavs\"]\n",
    "    audio_seconds_total += sum(len(w) for w in wavs) / TARGET_SR\n",
    "\n",
    "    inputs = processor(wavs, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.inference_mode():\n",
    "        logits = model(**inputs).logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    hyps.extend(normalize_text(t) for t in texts)\n",
    "\n",
    "wall = time.time() - t0\n",
    "rtf  = wall / audio_seconds_total\n",
    "print(f\"Items: {len(ds)}  audio: {audio_seconds_total/60:.2f}m  wall: {wall:.2f}s  RTF: {rtf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73fe6a46-b0d3-4b19-9940-f712507d351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 10.64%\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer\n",
    "score = wer(references, hyps) * 100.0\n",
    "print(f\"WER: {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "211f8bc4-1aa9-4049-911d-1aece9d9d1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: 262  audio: 32.41m  wall: 198.17s  RTF: 0.102\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "hyps = []\n",
    "audio_seconds_total = 0.0\n",
    "\n",
    "t0 = time.time()\n",
    "with torch.inference_mode():\n",
    "    for i in range(0, len(ds), BATCH_SIZE):\n",
    "        batch = ds.select(range(i, min(i + BATCH_SIZE, len(ds))))\n",
    "\n",
    "        wavs = [ex[\"audio\"][\"array\"] for ex in batch]        # list of 1-D arrays\n",
    "        audio_seconds_total += sum(len(w) for w in wavs) / TARGET_SR\n",
    "\n",
    "        inputs = processor(\n",
    "            wavs,\n",
    "            sampling_rate=TARGET_SR,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        logits = model(\n",
    "            input_values   = inputs.input_values,\n",
    "            attention_mask = inputs.attention_mask\n",
    "        ).logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        hyps.extend(normalize_text(t) for t in texts)\n",
    "\n",
    "wall = time.time() - t0\n",
    "rtf  = wall / audio_seconds_total\n",
    "print(f\"Items: {len(ds)}  audio: {audio_seconds_total/60:.2f}m  wall: {wall:.2f}s  RTF: {rtf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3548db00-b491-4586-9bb8-b03cd2ca8f18",
   "metadata": {},
   "source": [
    "import torch, numpy as np, re, time\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from jiwer import wer\n",
    "\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "TARGET_SR = 16_000\n",
    "SPLIT = \"test[:10%]\"   # adjust as you like\n",
    "\n",
    "# dataset + resample-on-the-fly\n",
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=SPLIT)\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=TARGET_SR))\n",
    "\n",
    "# normalizer used for WER\n",
    "def normalize_text(s): \n",
    "    s = s.lower()\n",
    "    s = re.sub(\"[^a-z' ]+\", \" \", s)\n",
    "    s = re.sub(\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "references = [normalize_text(x[\"text\"]) for x in ds]\n",
    "\n",
    "# models\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "hyps, audio_seconds_total = [], 0.0\n",
    "t0 = time.time()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i in range(0, len(ds), BATCH_SIZE):\n",
    "        batch = ds.select(range(i, min(i+BATCH_SIZE, len(ds))))\n",
    "        waves = [ex[\"audio\"][\"array\"] for ex in batch]           # list of 1-D float arrays\n",
    "        audio_seconds_total += sum(len(w) for w in waves) / TARGET_SR\n",
    "\n",
    "        inputs = processor(waves, sampling_rate=TARGET_SR,\n",
    "                           return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        logits = model(**inputs).logits                           # [B, T, vocab]\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        hyps.extend(normalize_text(t) for t in texts)\n",
    "\n",
    "wall = time.time() - t0\n",
    "rtf = wall / audio_seconds_total\n",
    "score = wer(references, hyps) * 100.0\n",
    "\n",
    "print(f\"Items: {len(ds)}  audio: {audio_seconds_total/60:.2f}m  wall: {wall:.2f}s  RTF: {rtf:.3f}\")\n",
    "print(f\"WER: {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87b38909-3e42-475a-85aa-988b401dfbc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wh_proc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Picking the SR this dataset was resampled towards\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m WHISPER_SR = \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mwh_proc\u001b[49m.feature_extractor, \u001b[33m\"\u001b[39m\u001b[33msampling_rate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m16000\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(WHISPER_SR, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBad sampling rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWHISPER_SR\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'wh_proc' is not defined"
     ]
    }
   ],
   "source": [
    "# Picking the SR this dataset was resampled towards\n",
    "WHISPER_SR = getattr(wh_proc.feature_extractor, \"sampling_rate\", 16000)\n",
    "assert isinstance(WHISPER_SR, (int, float)), f\"Bad sampling rate: {WHISPER_SR!r}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "192bc19b-f395-4be4-9d15-d74afe7dedf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WHISPER_SR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWHISPER_SR =\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mWHISPER_SR\u001b[49m, \u001b[38;5;28mtype\u001b[39m(WHISPER_SR))\n",
      "\u001b[31mNameError\u001b[39m: name 'WHISPER_SR' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"WHISPER_SR =\", WHISPER_SR, type(WHISPER_SR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "029e9e1b-0abb-4f7f-be8f-10dc30737b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper SR: 16000\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "WHISPER_ID  = \"openai/whisper-medium\"   # or \"small\"/\"base\" for speed\n",
    "wh_proc     = WhisperProcessor.from_pretrained(WHISPER_ID)\n",
    "wh_model    = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID).to(DEVICE).eval()\n",
    "\n",
    "# Whisper’s expected SR (should be 16000)\n",
    "WHISPER_SR = getattr(wh_proc.feature_extractor, \"sampling_rate\", 16000)\n",
    "assert isinstance(WHISPER_SR, int), f\"Bad sampling rate: {WHISPER_SR}\"\n",
    "print(\"Whisper SR:\", WHISPER_SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74aae2e5-f49a-44de-b579-35d0e7b04b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper SR: 16000\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import numpy as np, torch, time\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "WHISPER_ID = \"openai/whisper-medium\"   # or \"small\"/\"base\" for speed\n",
    "wh_proc  = WhisperProcessor.from_pretrained(WHISPER_ID)\n",
    "wh_model = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID).to(DEVICE).eval()\n",
    "\n",
    "WHISPER_SR = int(wh_proc.feature_extractor.sampling_rate)\n",
    "print(\"Whisper SR:\", WHISPER_SR)  # should print 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cff3f2d-3cec-4258-8e70-92eaee5629a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_wavs(examples):\n",
    "    # examples is a list of dataset rows (dicts)\n",
    "    wavs = [ex[\"audio\"][\"array\"].astype(\"float32\") for ex in examples]\n",
    "    lens = [len(w) for w in wavs]\n",
    "    return wavs, lens\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_wavs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8881bf3-d29c-4b30-94a1-057a5b08dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs, lens = next(iter(loader))\n",
    "assert isinstance(wavs, list) and len(wavs) > 0\n",
    "assert isinstance(wavs[0], np.ndarray) and wavs[0].ndim == 1\n",
    "assert len(wavs) == len(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff4f02af-b4ba-4ee9-81bb-695b7cead0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Whisper] batch 5/17 | items: 80 | wall: 65.7s | audio: 8.4m | RTF: 0.131\n",
      "[Whisper] batch 10/17 | items: 160 | wall: 138.4s | audio: 19.7m | RTF: 0.117\n",
      "[Whisper] batch 15/17 | items: 240 | wall: 203.7s | audio: 29.9m | RTF: 0.114\n",
      "[Whisper] batch 17/17 | items: 262 | wall: 225.7s | audio: 32.4m | RTF: 0.116\n",
      "\n",
      "[Whisper DONE] items: 262 | wall: 225.7s | audio: 32.4m | RTF: 0.116\n"
     ]
    }
   ],
   "source": [
    "import time, torch, numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Using a small/base model for CPU; switch back to \"openai/whisper-medium\".\n",
    "WHISPER_ID = \"openai/whisper-small\"\n",
    "wh_proc = WhisperProcessor.from_pretrained(WHISPER_ID)\n",
    "wh_model = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID).to(DEVICE).eval()\n",
    "\n",
    "# Whisper expects 16 kHz features internally\n",
    "WHISPER_SR = int(getattr(wh_proc.feature_extractor, \"sampling_rate\", 16000))\n",
    "\n",
    "# ---- Dataloader that returns lists of 1-D float arrays + lengths ----\n",
    "def collate_keep_lists(examples):\n",
    "    return {\n",
    "        \"wavs\":  [ex[\"audio\"][\"array\"] for ex in examples],  # list[np.ndarray], dtype float32, 1-D\n",
    "        \"lens\":  [ex[\"audio\"][\"array\"].shape[0] for ex in examples],  # sample counts\n",
    "        \"text\":  [ex[\"text\"] for ex in examples],\n",
    "    }\n",
    "\n",
    "BATCH_SIZE = 16  # 8–32 is fine on CPU for small/base; lower if you see OOM\n",
    "loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_keep_lists)\n",
    "\n",
    "# Beam search config – set num_beams=1 for greedy (faster, fewer stalls)\n",
    "GEN_KW = dict(\n",
    "    task=\"transcribe\",          # or \"translate\"\n",
    "    language=\"en\",              # set explicitly if needed\n",
    "    num_beams=1,                # <-- greedy = fastest\n",
    "    max_new_tokens=225,         # cap generated length; adjust if needed\n",
    "    length_penalty=1.0,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "hypotheses = []\n",
    "audio_seconds_total = 0.0\n",
    "t0 = time.time()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for bi, batch in enumerate(loader, 1):\n",
    "        wavs = batch[\"wavs\"]             # list of 1-D float arrays\n",
    "        lens = batch[\"lens\"]             # lengths in input sampling rate\n",
    "        refs = batch[\"text\"]\n",
    "\n",
    "        # Updating RTF denominator using *model* SR\n",
    "        audio_seconds_total += sum(l for l in lens) / WHISPER_SR\n",
    "\n",
    "        # Processor computes log-mels and attention_mask; pad to longest in the batch\n",
    "        inputs = wh_proc(\n",
    "            wavs,\n",
    "            sampling_rate=WHISPER_SR,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        input_features = inputs[\"input_features\"].to(DEVICE)\n",
    "        attention_mask = inputs.get(\"attention_mask\")  # present in recent transformers\n",
    "\n",
    "        # Generating ids (pass attention_mask to avoid ‘pad==eos’ pathology)\n",
    "        gen_ids = wh_model.generate(\n",
    "            input_features,\n",
    "            attention_mask=attention_mask,   # <-- IMPORTANT on padded batches\n",
    "            **GEN_KW\n",
    "        )\n",
    "\n",
    "        texts = wh_proc.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        hypotheses.extend(texts)\n",
    "\n",
    "        # progress heartbeat\n",
    "        if bi % 5 == 0 or bi == len(loader):\n",
    "            wall = time.time() - t0\n",
    "            rtf = wall / max(audio_seconds_total, 1e-6)\n",
    "            print(f\"[Whisper] batch {bi}/{len(loader)} | items: {len(hypotheses)} | wall: {wall:,.1f}s | audio: {audio_seconds_total/60:,.1f}m | RTF: {rtf:.3f}\")\n",
    "\n",
    "wall = time.time() - t0\n",
    "rtf = wall / max(audio_seconds_total, 1e-6)\n",
    "print(f\"\\n[Whisper DONE] items: {len(hypotheses)} | wall: {wall:,.1f}s | audio: {audio_seconds_total/60:,.1f}m | RTF: {rtf:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cfb5acf-2ded-44bd-853e-885f408ae9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Whisper]  WER: 22.75%  CER: 5.63%\n"
     ]
    }
   ],
   "source": [
    "# 'references' should be your ground-truth list, already normalized the same way\n",
    "from jiwer import wer, cer\n",
    "\n",
    "wh_wer = wer(references, hypotheses) * 100\n",
    "wh_cer = cer(references, hypotheses) * 100\n",
    "print(f\"[Whisper]  WER: {wh_wer:.2f}%  CER: {wh_cer:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07dd04db-c799-4fe7-911a-55cc76f09bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] CSV + JSON under runs/ with timestamp 20250904-132637\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, json, time, os\n",
    "\n",
    "# 1) Make sure we expose the hyps with the expected name\n",
    "wh_hyps = hypotheses  # <-- 'hypotheses' came from your Whisper loop\n",
    "\n",
    "# 2) If your timing variables have different names, map them here:\n",
    "# wall = wh_wall\n",
    "# rtf  = wh_rtf\n",
    "\n",
    "# 3) Build a compact summary of the run\n",
    "run = {\n",
    "    \"model\": \"whisper-medium\",        # adjust if you used a different size\n",
    "    \"beam\": 5,                        # match your GEN_KW settings\n",
    "    \"length_penalty\": 1.0,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"dataset\": \"LibriSpeech test-clean (slice)\",  # or your exact split label\n",
    "    \"items\": len(references),\n",
    "    \"wall_s\": wall,                   # total wall time (seconds)\n",
    "    \"audio_s\": audio_seconds_total,   # total audio (seconds)\n",
    "    \"rtf\": rtf,                       # real-time factor\n",
    "    \"wer\": wh_wer / 100.0,            # store as fraction (e.g., 0.2275)\n",
    "    \"cer\": wh_cer / 100.0             # store as fraction (e.g., 0.0563)\n",
    "}\n",
    "\n",
    "# 4) Building a pairwise ref/hyp table (useful for error analysis)\n",
    "df = pd.DataFrame({\n",
    "    \"ref\": references,\n",
    "    \"hyp\": wh_hyps\n",
    "})\n",
    "\n",
    "# 5) Write artifacts\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_csv(f\"runs/whisper_medium_librispeech_{ts}.csv\", index=False)\n",
    "\n",
    "with open(f\"runs/whisper_medium_librispeech_{ts}.json\", \"w\") as f:\n",
    "    json.dump(run, f, indent=2)\n",
    "\n",
    "print(f\"[Saved] CSV + JSON under runs/ with timestamp {ts}\")\n",
    "# ---- end block ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a2a3b74-f623-40ed-be92-801e18050412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa  # only needed if you ever get filepaths\n",
    "\n",
    "TARGET_SR = 16000  # make sure this matches your pipeline\n",
    "\n",
    "def extract_wavs_and_lens(batch, target_sr=TARGET_SR):\n",
    "    \"\"\"\n",
    "    Return: wavs (list of 1-D float arrays), lens (list of ints)\n",
    "    Handles several common DataLoader shapes:\n",
    "      - dict with \"wavs\"\n",
    "      - dict with \"audio\" (list of dicts or arrays)\n",
    "      - list of examples where each ex[\"audio\"][\"array\"] exists\n",
    "      - (wavs, lens) tuple prepared by a collate_fn\n",
    "      - paths (rare) -> will load with librosa\n",
    "    \"\"\"\n",
    "    # 1) dict batch (HuggingFace default)\n",
    "    if isinstance(batch, dict):\n",
    "        if \"wavs\" in batch:                               # custom collate_keep_lists\n",
    "            wavs = batch[\"wavs\"]\n",
    "        elif \"audio\" in batch:                            # HF Datasets default\n",
    "            aud = batch[\"audio\"]\n",
    "            if isinstance(aud, list):\n",
    "                # list of dicts or arrays\n",
    "                wavs = [\n",
    "                    (a[\"array\"] if isinstance(a, dict) and \"array\" in a else np.asarray(a, dtype=np.float32))\n",
    "                    for a in aud\n",
    "                ]\n",
    "            elif isinstance(aud, dict) and \"array\" in aud:\n",
    "                wavs = [aud[\"array\"]]\n",
    "            else:\n",
    "                raise TypeError(f\"Unsupported 'audio' field type: {type(aud)}\")\n",
    "        else:\n",
    "            raise KeyError(\"Batch dict has neither 'wavs' nor 'audio'\")\n",
    "        lens = [len(w) for w in wavs]\n",
    "        return wavs, lens\n",
    "\n",
    "    # 2) list of examples, each a dict\n",
    "    if isinstance(batch, list) and len(batch) > 0 and isinstance(batch[0], dict):\n",
    "        wavs = [\n",
    "            (ex[\"audio\"][\"array\"] if isinstance(ex[\"audio\"], dict) else np.asarray(ex[\"audio\"], dtype=np.float32))\n",
    "            for ex in batch\n",
    "        ]\n",
    "        lens = [len(w) for w in wavs]\n",
    "        return wavs, lens\n",
    "\n",
    "    # 3) collate_fn already produced (wavs, lens)\n",
    "    if isinstance(batch, (tuple, list)) and len(batch) == 2:\n",
    "        wavs, lens = batch\n",
    "        return wavs, lens\n",
    "\n",
    "    # 4) paths (rare)\n",
    "    if isinstance(batch, list) and len(batch) > 0 and isinstance(batch[0], str):\n",
    "        wavs = [librosa.load(p, sr=target_sr)[0] for p in batch]\n",
    "        lens = [len(w) for w in wavs]\n",
    "        return wavs, lens\n",
    "\n",
    "    raise TypeError(f\"Don't know how to read batch of type {type(batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e34e8e87-c836-4642-af18-75d412f964ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Wav2Vec2] WER: 10.64% CER: 2.33% RTF: 0.169\n"
     ]
    }
   ],
   "source": [
    "# Lists to store predictions\n",
    "w2v_hyps = []\n",
    "audio_seconds_total = 0.0\n",
    "t0 = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        wavs, lens = extract_wavs_and_lens(batch)  # <-- robust extraction\n",
    "        audio_seconds_total += sum(lens) / TARGET_SR\n",
    "\n",
    "        inputs = processor(\n",
    "            wavs, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "        logits = model(**inputs).logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        w2v_hyps.extend([t.lower().strip() for t in texts])  # normalize\n",
    "\n",
    "w2v_wall = time.time() - t0\n",
    "w2v_rtf  = w2v_wall / audio_seconds_total\n",
    "\n",
    "from jiwer import wer, cer\n",
    "w2v_wer = wer(references, w2v_hyps) * 100\n",
    "w2v_cer = cer(references, w2v_hyps) * 100\n",
    "\n",
    "print(f\"[Wav2Vec2] WER: {w2v_wer:.2f}% CER: {w2v_cer:.2f}% RTF: {w2v_rtf:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b98284-da48-43bd-8670-4b36035f0af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (audio)",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
